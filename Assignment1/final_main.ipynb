{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from LogisticRegression import logistic_regression\n",
    "from LRM import logistic_regression_multiclass\n",
    "from DataReader import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "train_filename = \"training.npz\"\n",
    "test_filename = \"test.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features(X, y, plot_name):\n",
    "    '''This function is used to plot a 2-D scatter plot of training features. \n",
    "\n",
    "    Args:\n",
    "        X: An array of shape [n_samples, 2].\n",
    "        y: An array of shape [n_samples,]. Only contains 1 or -1.\n",
    "\n",
    "    Returns:\n",
    "        No return. Save the plot to 'train_features.*' and include it\n",
    "        in submission.\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "    plt.rcParams.update({'figure.figsize':(6,6), 'figure.dpi':100})\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    classes = [\"-1\",\"1\"]\n",
    "    print(X.shape, y.shape)\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.title(plot_name)\n",
    "    plt.xlabel(\"Symmetry\")\n",
    "    plt.ylabel(\"Intensity\")\n",
    "    # plt.legend(handles=scatter.legend_elements()[0], labels=classes)\n",
    "    # plt.show()\n",
    "    plt.savefig(\"./plots/\"+plot_name+\".png\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_result(X, y, W, plot_name):\n",
    "\n",
    "    plt.rcParams.update({'figure.figsize':(6, 6), 'figure.dpi':100})\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "    print(X.shape, y.shape)\n",
    "    b = W[0]\n",
    "    w1, w2 = W[1], W[2]\n",
    "\n",
    "    c = -b/w2\n",
    "    m = -w1/w2\n",
    "    xmin, xmax = min(X[:, 0]), max(X[:, 0])\n",
    "    ymin, ymax = min(X[:, 1]), max(X[:, 1])\n",
    "\n",
    "    xd = np.array([xmin, xmax])\n",
    "    yd = m*xd + c\n",
    "    # plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
    "\n",
    "    scatter = plt.scatter(X[:,0], X[:,1], c= y)\n",
    "    # plot(xd, yd, 'go--', linewidth=2, markersize=12)\n",
    "    plt.plot(xd, yd, color='green', marker='o', linestyle='dashed', linewidth=2, markersize=12)\n",
    "    # plt.legend(handles=scatter.legend_elements()[0], labels=classes)\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)\n",
    "\n",
    "    plt.title(plot_name)\n",
    "    plt.xlabel(\"Symmetry\")\n",
    "    plt.ylabel(\"Intensity\")\n",
    "    # plt.legend(handles=scatter.legend_elements()[0], labels=classes)\n",
    "    # plt.show()\n",
    "    plt.savefig('./plots/'+plot_name+'.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multi(X, W):\n",
    "    \"\"\"Predict class labels for samples in X.\n",
    "\n",
    "    Args:\n",
    "        X: An array of shape [n_samples, n_features].\n",
    "\n",
    "    Returns:\n",
    "        preds: An array of shape [n_samples,]. Only contains 0,..,k-1.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    y_temp = np.matmul(X, W)\n",
    "    y_hat = np.array([softmax(y_temp[i]) for i in range(y_temp.shape[0])])\n",
    "    prediction = np.argmax(y_hat , axis=1)\n",
    "    return prediction\n",
    "    ### END YOUR CODE\n",
    "\n",
    "def softmax_multi(x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        ### You must implement softmax by youself, otherwise you will not get credits for this part.\n",
    "\n",
    "\t\t### YOUR CODE HERE\n",
    "        exps = np.exp(x)\n",
    "        smax = exps/np.sum(exps)\n",
    "        # print(\"Softmax shape: \", smax.shape)\n",
    "        return smax\n",
    "\t\t### END YOUR CODE\n",
    "\n",
    "def score_multi(X, labels, W):\n",
    "    \"\"\"Returns the mean accuracy on the given test data and labels.\n",
    "\n",
    "    Args:\n",
    "        X: An array of shape [n_samples, n_features].\n",
    "        labels: An array of shape [n_samples,]. Only contains 0,..,k-1.\n",
    "\n",
    "    Returns:\n",
    "        score: An float. Mean accuracy of self.predict(X) wrt. labels.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    preds = predict_test(X)\n",
    "    n_samples = len(preds)\n",
    "    return np.sum(preds == labels)/n_samples\n",
    "    ### END YOUR CODE\n",
    "\n",
    "def predict_test(X, W):\n",
    "    \"\"\"Predict class labels for samples in X.\n",
    "\n",
    "    Args:\n",
    "        X: An array of shape [n_samples, n_features].\n",
    "\n",
    "    Returns:\n",
    "        preds: An array of shape [n_samples,]. Only contains 1 or -1.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    sigmoid_preds = 1 / (1 + np.exp(-1*np.matmul(X, W)))\n",
    "    preds = [-1 if p < 0.5 else 1 for p in sigmoid_preds]\n",
    "\n",
    "    return preds\n",
    "    ### END YOUR CODE\n",
    "\n",
    "def score_test(X, y, W):\n",
    "    \"\"\"Returns the mean accuracy on the given test data and labels.\n",
    "\n",
    "    Args:\n",
    "        X: An array of shape [n_samples, n_features].\n",
    "        y: An array of shape [n_samples,]. Only contains 1 or -1.\n",
    "\n",
    "    Returns:\n",
    "        score: An float. Mean accuracy of self.predict(X) wrt. y.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    preds = predict_test(X, W)\n",
    "    n_samples = len(preds)\n",
    "\n",
    "    return np.sum(preds == y)/n_samples\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, labels = load_data(os.path.join(data_dir, train_filename))\n",
    "raw_train, raw_valid, label_train, label_valid = train_valid_split(raw_data, labels, 2300)\n",
    "\n",
    "##### Preprocess raw data to extract features\n",
    "train_X_all = prepare_X(raw_train)\n",
    "valid_X_all = prepare_X(raw_valid)\n",
    "##### Preprocess labels for all data to 0,1,2 and return the idx for data from '1' and '2' class.\n",
    "train_y_all, train_idx = prepare_y(label_train)\n",
    "valid_y_all, val_idx = prepare_y(label_valid)  \n",
    "\n",
    "####### For binary case, only use data from '1' and '2'  \n",
    "train_X = train_X_all[train_idx]\n",
    "train_y = train_y_all[train_idx]\n",
    "####### Only use the first 1350 data examples for binary training. \n",
    "train_X = train_X[0:1350]\n",
    "train_y = train_y[0:1350]\n",
    "\n",
    "valid_X = valid_X_all[val_idx]\n",
    "valid_y = valid_y_all[val_idx]\n",
    "####### set lables to  1 and -1. Here convert label '2' to '-1' which means we treat data '1' as postitive class. \n",
    "train_y[np.where(train_y==2)] = -1\n",
    "valid_y[np.where(valid_y==2)] = -1\n",
    "data_shape= train_y.shape[0] \n",
    "# print(train_X.shape, train_y.shape, valid_X.shape, valid_y.shape)\n",
    "\n",
    "# Visualize training data.\n",
    "# visualize_features(train_X[:, 1:3], train_y, \"2_class_train_features\")\n",
    "\n",
    "#######------------Logistic Regression Sigmoid Case------------\n",
    "\n",
    "#### Check BGD, SGD, miniBGD\n",
    "# logisticR_classifier = logistic_regression(learning_rate=0.4, max_iter=500)\n",
    "\n",
    "# logisticR_classifier.fit_BGD(train_X, train_y)\n",
    "# params = logisticR_classifier.get_params()\n",
    "# print(\"Final Weights: \", params)\n",
    "# print(\"Accuracy: \", logisticR_classifier.score(train_X, train_y))\n",
    "# visualize_result(train_X[:, 1:3], train_y, params)\n",
    "# visualize_result(valid_X[:, 1:3], valid_y, params)\n",
    "\n",
    "# print(\"data_shape: \", data_shape)\n",
    "# logisticR_classifier.fit_miniBGD(train_X, train_y, data_shape)\n",
    "# params = logisticR_classifier.get_params()\n",
    "# print(\"Final Weights: \", params)\n",
    "# print(\"Accuracy: \", logisticR_classifier.score(train_X, train_y))\n",
    "# visualize_result(train_X[:, 1:3], train_y, params)\n",
    "\n",
    "# logisticR_classifier.fit_miniBGD(train_X, train_y, 16)\n",
    "# print(\"Final Weights: \", logisticR_classifier.get_params())\n",
    "# print(\"Accuracy: \", logisticR_classifier.score(train_X, train_y))\n",
    "# visualize_result(train_X[:, 1:3], train_y, params)\n",
    "# visualize_result(valid_X[:, 1:3], valid_y, params)\n",
    "\n",
    "# logisticR_classifier.fit_miniBGD(train_X, train_y, 16)\n",
    "# print(\"Final Weights: \", logisticR_classifier.get_params())\n",
    "# print(\"Accuracy: \", logisticR_classifier.score(train_X, train_y))\n",
    "# visualize_result(train_X[:, 1:3], train_y, params)\n",
    "# visualize_result(valid_X[:, 1:3], valid_y, params)\n",
    "\n",
    "# logisticR_classifier.fit_miniBGD(train_X, train_y, 1)\n",
    "# print(\"Final Weights: \", logisticR_classifier.get_params())\n",
    "# print(\"Accuracy: \", logisticR_classifier.score(train_X, train_y))\n",
    "\n",
    "# logisticR_classifier.fit_SGD(train_X, train_y)\n",
    "# params = logisticR_classifier.get_params()\n",
    "# print(\"Final Weights: \", params)\n",
    "# print(\"Accuracy: \", logisticR_classifier.score(train_X, train_y))\n",
    "# visualize_result(train_X[:, 1:3], train_y, params)\n",
    "# visualize_result(valid_X[:, 1:3], valid_y, params)\n",
    "\n",
    "# Explore different hyper-parameters.\n",
    "# ### YOUR CODE HERE\n",
    "# final_model = [float('-inf'), [], 0, 0, 0, '']\n",
    "# mini_bgd_final_model = [float('-inf'), [], 0, 0, 0, '']\n",
    "# bgd_final_model = [float('-inf'), [], 0, 0, 0, '']\n",
    "# sgd_final_model = [float('-inf'), [], 0, 0, 0, '']\n",
    "\n",
    "\n",
    "# lrs = [0.1, 0.2, 0.4, 0.75, 1.0]\n",
    "# max_iters = [50, 100, 200, 400, 500]\n",
    "# gd_types = ['BGD', 'miniBGD', 'SGD']\n",
    "\n",
    "# for gd_type in gd_types:\n",
    "#     if gd_type == 'miniBGD':\n",
    "#         mini_batch_sizes = [16, 32, 64]\n",
    "#         for lr in lrs:\n",
    "#             for max_iter in max_iters:\n",
    "#                 logisticR_classifier = logistic_regression(learning_rate=lr, max_iter=max_iter)\n",
    "#                 for mini_batch_size in mini_batch_sizes:\n",
    "#                     print('miniBGD', lr, max_iter, mini_batch_size)\n",
    "#                     logisticR_classifier.fit_miniBGD(train_X, train_y, mini_batch_size)\n",
    "#                     params = logisticR_classifier.get_params()\n",
    "#                     training_score = logisticR_classifier.score(train_X, train_y)\n",
    "#                     print(\"training_score: \", training_score)\n",
    "#                     validation_score = logisticR_classifier.score(valid_X, valid_y)\n",
    "#                     print(\"validation_score: \", validation_score)\n",
    "#                     if validation_score > mini_bgd_final_model[0]:\n",
    "#                         mini_bgd_final_model = [validation_score, params, lr, max_iter, mini_batch_size, 'miniBGD']\n",
    "#                         print(mini_bgd_final_model)\n",
    "#     elif gd_type == 'BGD':\n",
    "#         for lr in lrs:\n",
    "#             for max_iter in max_iters:\n",
    "#                 logisticR_classifier = logistic_regression(learning_rate=lr, max_iter=max_iter)\n",
    "#                 print('BGD', lr, max_iter)\n",
    "#                 logisticR_classifier.fit_BGD(train_X, train_y)\n",
    "#                 params = logisticR_classifier.get_params()\n",
    "#                 training_score = logisticR_classifier.score(train_X, train_y)\n",
    "#                 print(\"training_score: \", training_score)\n",
    "#                 validation_score = logisticR_classifier.score(valid_X, valid_y)\n",
    "#                 print(\"validation_score: \", validation_score)\n",
    "#                 if validation_score > bgd_final_model[0]:\n",
    "#                     bgd_final_model = [validation_score, params, lr, max_iter, 0, 'BGD']\n",
    "#                     print(bgd_final_model)\n",
    "#     elif gd_type == 'SGD':\n",
    "#         for lr in lrs:\n",
    "#             for max_iter in max_iters:\n",
    "#                 logisticR_classifier = logistic_regression(learning_rate=lr, max_iter=max_iter)\n",
    "#                 print('SGD', lr, max_iter)\n",
    "#                 logisticR_classifier.fit_SGD(train_X, train_y)\n",
    "#                 params = logisticR_classifier.get_params()\n",
    "#                 training_score = logisticR_classifier.score(train_X, train_y)\n",
    "#                 print(\"training_score: \", training_score)\n",
    "#                 validation_score = logisticR_classifier.score(valid_X, valid_y)\n",
    "#                 print(\"validation_score: \", validation_score)\n",
    "#                 if validation_score > sgd_final_model[0]:\n",
    "#                     sgd_final_model = [validation_score, params, lr, max_iter, 0, 'SGD']\n",
    "#                     print(sgd_final_model)\n",
    "\n",
    "# print(\"bgd_final_model: \", bgd_final_model)\n",
    "# print(\"mini_bgd_final_model: \", mini_bgd_final_model)\n",
    "# print(\"sgd_final_model: \", sgd_final_model)\n",
    "\n",
    "# #Best models for each algorithm\n",
    "# bgd_final_model = [0.9735449735449735, [ 0.46873881, 10.41498126, -4.74745088], 1.0, 400, 0, 'BGD']\n",
    "# mini_bgd_final_model = [0.9788359788359788, [ 1.70682355 17.09388503 -5.49211733], 0.4, 300, 64, 'miniBGD']\n",
    "# sgd_final_model = [0.9788359788359788, [ 9.4833702 , 29.21028412,  1.15452184], 0.1, 200, 0, 'SGD']\n",
    "# ### END YOUR CODE\n",
    "\n",
    "# # Visualize the your 'best' model after training.\n",
    "\n",
    "# logisticR_classifier = logistic_regression(learning_rate=1.0, max_iter=400)\n",
    "\n",
    "# logisticR_classifier.fit_BGD(train_X, train_y)\n",
    "# params = logisticR_classifier.get_params()\n",
    "# print(\"Final Weights: \", params)\n",
    "# print(\"Training Accuracy: \", logisticR_classifier.score(train_X, train_y))\n",
    "# print(\"Validation Accuracy: \", logisticR_classifier.score(valid_X, valid_y))\n",
    "# Final Weights:  [ 0.47071328 10.41367613 -4.74328509]\n",
    "# Training Accuracy:  0.9666666666666667\n",
    "# Validation Accuracy:  0.9735449735449735\n",
    "# visualize_result(train_X[:, 1:3], train_y, params)\n",
    "# visualize_result(valid_X[:, 1:3], valid_y, params)\n",
    "\n",
    "# logisticR_classifier = logistic_regression(learning_rate=0.4, max_iter=300)\n",
    "# logisticR_classifier.fit_miniBGD(train_X, train_y, 64)\n",
    "# params = logisticR_classifier.get_params()\n",
    "# print(\"Final Weights: \", params)\n",
    "# print(\"Training Accuracy: \", logisticR_classifier.score(train_X, train_y))\n",
    "# print(\"Validation Accuracy: \", logisticR_classifier.score(valid_X, valid_y))\n",
    "# Final Weights:  [ 2.5210473  19.05145317 -5.20721508]\n",
    "# Training Accuracy:  0.9696296296296296\n",
    "# Validation Accuracy:  0.9761904761904762\n",
    "\n",
    "# logisticR_classifier = logistic_regression(learning_rate=0.1, max_iter=200)\n",
    "# logisticR_classifier.fit_SGD(train_X, train_y)\n",
    "# params = logisticR_classifier.get_params()\n",
    "# print(\"Final Weights: \", params)\n",
    "# print(\"Training Accuracy: \", logisticR_classifier.score(train_X, train_y))\n",
    "# print(\"Validation Accuracy: \", logisticR_classifier.score(valid_X, valid_y))\n",
    "# Training Accuracy:  0.9725925925925926\n",
    "# Validation Accuracy:  0.9788359788359788\n",
    "# visualize_result(train_X[:, 1:3], train_y, params, \"bgd_final_model_train_result_sigmoid\")\n",
    "# visualize_result(train_X[:, 1:3], train_y, params, \"minibgd_final_model_train_result_sigmoid\")\n",
    "# visualize_result(train_X[:, 1:3], train_y, params, \"sgd_final_model_train_result_sigmoid\")\n",
    "# visualize_result(valid_X[:, 1:3], valid_y, params, \"bgd_final_model_validation_result_sigmoid\")\n",
    "# visualize_result(valid_X[:, 1:3], valid_y, params, \"minibgd_final_model_validation_result_sigmoid\")\n",
    "# visualize_result(valid_X[:, 1:3], valid_y, params, \"sgd_final_model_validation_result_sigmoid\")\n",
    "\n",
    "# Best model is the mini BGD model\n",
    "# best_model = [0.9788359788359788, [ 1.70682355 17.09388503 -5.49211733], 0.4, 300, 64, 'miniBGD']\n",
    "# best_params = best_model[1]\n",
    "# visualize_result(train_X[:, 1:3], train_y, best_params, \"best_model_training_loss\")\n",
    "# visualize_result(valid_X[:, 1:3], valid_y, best_params, \"best_model_validation_loss\")\n",
    "\n",
    "\n",
    "# ### Use the 'best' model above to do testing. Note that the test data should be loaded and processed in the same way as the training data.\n",
    "# ## YOUR CODE HERE\n",
    "# raw_data_test, labels_test = load_data(os.path.join(data_dir, test_filename))\n",
    "\n",
    "# # # # # # ##### Preprocess raw data to extract features\n",
    "# test_X_all = prepare_X(raw_data_test)\n",
    "# ##### Preprocess labels for all data to 0,1,2 and return the idx for data from '1' and '2' class.\n",
    "# test_y_all, test_idx = prepare_y(labels_test)\n",
    "# ####### For binary case, only use data from '1' and '2'  \n",
    "# test_X = test_X_all[test_idx]\n",
    "# test_y = test_y_all[test_idx]\n",
    "# ####### set lables to  1 and -1. Here convert label '2' to '-1' which means we treat data '1' as postitive class. \n",
    "# test_y[np.where(test_y==2)] = -1\n",
    "# test_data_shape= test_y.shape[0]\n",
    "# # print(\"test_data_shape: \", test_data_shape)\n",
    "# logisticR_classifier = logistic_regression(learning_rate=0.4, max_iter=300)\n",
    "# logisticR_classifier.fit_miniBGD(train_X, train_y, 64)\n",
    "# best_params = logisticR_classifier.get_params()\n",
    "\n",
    "# print(\"Final Weights: \", best_params )\n",
    "# print(\"Training Accuracy: \", score_test(train_X, train_y, best_params))\n",
    "# print(\"Validation Accuracy: \", score_test(valid_X, valid_y, best_params))\n",
    "# print(\"Test Accuracy: \", score_test(test_X, test_y, best_params))\n",
    "\n",
    "\n",
    "# visualize_result(test_X[:, 1:3], test_y, best_params, \"best_model_testing_result\")\n",
    "# visualize_features(test_X[:, 1:3], test_y)\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result_multi(X, y, W):\n",
    "    plt.rcParams.update({'figure.figsize':(6, 6), 'figure.dpi':100})\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "    print(X.shape, y.shape, W.shape)\n",
    "\n",
    "    xmin, xmax = min(X[:, 0]), max(X[:, 0])\n",
    "    ymin, ymax = min(X[:, 1]), max(X[:, 1])\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.scatter(X[:,0], X[:,1], c= y)\n",
    "    \n",
    "    color = ['red', 'green', 'black']\n",
    "    \n",
    "    for i in range(2):\n",
    "        # w = W[i]\n",
    "        # b = w[0]\n",
    "        # w1, w2 = w[1], w[2]\n",
    "        b, w1, w2 = W[0][i], W[1][i], W[2][i]\n",
    "        c = -b/w2\n",
    "        m = -w1/w2\n",
    "\n",
    "        xd = np.array([xmin, xmax])\n",
    "        yd = m*xd + c\n",
    "        plt.plot(xd, yd, color=color[i], marker='o', linestyle='dashed', linewidth=2, markersize=12)\n",
    "\n",
    "\n",
    "    plt.title(\"MultiClass result\")\n",
    "    plt.xlabel(\"Symmetry\")\n",
    "    plt.ylabel(\"Intensity\")\n",
    "    # plt.show()\n",
    "    plt.savefig('./plots/2class_softmax_convergence.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------Logistic Regression Multiple-class case, let k= 3------------\n",
    "###### Use all data from '0' '1' '2' for training\n",
    "# train_X = train_X_all\n",
    "# train_y = train_y_all\n",
    "# valid_X = valid_X_all\n",
    "# valid_y = valid_y_all\n",
    "\n",
    "# raw_data_test, labels_test = load_data(os.path.join(data_dir, test_filename))\n",
    "# test_X = prepare_X(raw_data_test)\n",
    "# test_y, idx = prepare_y(labels_test)\n",
    "\n",
    "# visualize_features(train_X[:, 1:3], train_y, \"LRM_train_features\")\n",
    "# visualize_features(valid_X[:, 1:3], valid_y, \"LRM_valid_features\")\n",
    "# visualize_features(test_X[:, 1:3], test_y, \"LRM_test_features\")\n",
    "\n",
    "########  miniBGD for multiclass Logistic Regression\n",
    "# print(train_X.shape, train_y.shape, valid_X.shape, valid_y.shape)\n",
    "# logisticR_classifier_multiclass = logistic_regression_multiclass(learning_rate=0.5, max_iter=100,  k= 3)\n",
    "# logisticR_classifier_multiclass.fit_miniBGD(train_X, train_y, 32)\n",
    "# params = logisticR_classifier_multiclass.get_params()\n",
    "\n",
    "# Explore different hyper-parameters.\n",
    "### YOUR CODE HERE\n",
    "# final_model = [float('-inf'), [], 0, 0, 0, '']\n",
    "\n",
    "# lrs = [0.1, 0.2, 0.4, 0.75, 1.0]\n",
    "# max_iters = [50, 100, 200, 400, 500]\n",
    "# mini_batch_sizes = [16, 32, 64]\n",
    "\n",
    "# for lr in lrs:\n",
    "#     for max_iter in max_iters:\n",
    "#         logisticR_classifier_multiclass = logistic_regression_multiclass(learning_rate=lr, max_iter=max_iter,  k= 3)\n",
    "#         for mini_batch_size in mini_batch_sizes:\n",
    "#             print('miniBGD', lr, max_iter, mini_batch_size)\n",
    "#             logisticR_classifier_multiclass.fit_miniBGD(train_X, train_y, mini_batch_size)\n",
    "#             params = logisticR_classifier_multiclass.get_params()\n",
    "#             training_score = logisticR_classifier_multiclass.score(train_X, train_y)\n",
    "#             print(\"training_score: \", training_score)\n",
    "#             validation_score = logisticR_classifier_multiclass.score(valid_X, valid_y)\n",
    "#             print(\"validation_score: \", validation_score)\n",
    "#             if validation_score > final_model[0]:\n",
    "#                 final_model = [validation_score, params, lr, max_iter, mini_batch_size, 'miniBGD']\n",
    "#                 print(final_model)\n",
    "#print(\"Final Model: \", final_model)\n",
    "\n",
    "# final_model = [0.8857142857142857,\n",
    "# [[  8.73615057,   1.29160035,  -3.41908028],\n",
    "# [ -3.0697722 ,  12.54171746, -19.47054993],\n",
    "# [ 13.91399786,  -5.0503584 ,   1.61281812]],1.0,200,32,'miniBGD']\n",
    "\n",
    "# logisticR_classifier_multiclass = logistic_regression_multiclass(learning_rate=0.5, max_iter=200,  k= 3)\n",
    "# logisticR_classifier_multiclass.fit_miniBGD(train_X, train_y, 128)\n",
    "# params = logisticR_classifier_multiclass.get_params()\n",
    "# print(\"Params: \", params)\n",
    "# training_score = logisticR_classifier_multiclass.score(train_X, train_y)\n",
    "# print(\"training_score: \", training_score)\n",
    "# validation_score = logisticR_classifier_multiclass.score(valid_X, valid_y)\n",
    "# print(\"validation_score: \", validation_score)\n",
    "# testing_score = logisticR_classifier_multiclass.score(test_X, test_y)\n",
    "# print(\"testing_score: \", testing_score)\n",
    "\n",
    "# Params:  [[  7.18726599  -0.984131    -4.67416801]\n",
    "#  [  0.55830733  13.9652507  -15.0892574 ]\n",
    "#  [ 13.12298073  -5.90900813   1.11319128]]\n",
    "# training_score:  0.8947826086956522\n",
    "# validation_score:  0.8793650793650793\n",
    "# testing_score:  0.8672350791717418\n",
    "\n",
    "# Params:  [[  6.32856892  -1.53839471  -6.19131709]\n",
    "#  [  0.91083148  16.55917569 -15.3376584 ]\n",
    "#  [ 10.83608674  -7.96171329  -1.29303694]]\n",
    "# training_score:  0.8995652173913044\n",
    "# validation_score:  0.873015873015873\n",
    "# testing_score:  0.8684531059683313\n",
    "\n",
    "# logisticR_classifier_multiclass = logistic_regression_multiclass(learning_rate=0.6, max_iter=200,  k= 3)\n",
    "# logisticR_classifier_multiclass.fit_miniBGD(train_X, train_y, 64)\n",
    "# Params:  [[  9.64785501   1.52456229  -2.22797855]\n",
    "#  [  1.66443816  14.30584618 -13.72638955]\n",
    "#  [ 11.74981459  -6.85746315  -0.08700084]]\n",
    "# training_score:  0.8956521739130435\n",
    "# validation_score:  0.873015873015873\n",
    "# testing_score:  0.8708891595615104\n",
    "\n",
    "# logisticR_classifier_multiclass = logistic_regression_multiclass(learning_rate=0.5, max_iter=200,  k= 3)\n",
    "# logisticR_classifier_multiclass.fit_miniBGD(train_X, train_y, 128)\n",
    "# Params:  [[  8.20477456   0.44883913  -2.3851905 ]\n",
    "#  [  2.15294719  11.5409138  -11.45329729]\n",
    "#  [  9.05297781  -7.93059492  -1.66585129]]\n",
    "# training_score:  0.8921739130434783\n",
    "# validation_score:  0.8761904761904762\n",
    "# testing_score:  0.8745432399512789\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE\n",
    "\n",
    "# Visualize the your 'best' model after training.\n",
    "# visualize_result_multi(train_X[:, 1:3], train_y, params)\n",
    "# visualize_result_multi(valid_X[:, 1:3], valid_y, params)\n",
    "\n",
    "\n",
    "# Use the 'best' model above to do testing.\n",
    "### YOUR CODE HERE\n",
    "# testing_score = logisticR_classifier_multiclass.score(test_X, test_y)\n",
    "# print(\"testing_score: \", testing_score)\n",
    "# visualize_result_multi(test_X[:, 1:3], test_y, params)\n",
    "### END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after 0 mini-batch: [[ 0.00351563 -0.00351563]\n",
      " [-0.01191377  0.01191377]\n",
      " [ 0.00429995 -0.00429995]]\n",
      "W2 - W1 after 0 mini-batch: [-0.00703125  0.02382753 -0.00859989]\n",
      "Gradient/Weight:  [[-10. -10.]\n",
      " [-10. -10.]\n",
      " [-10. -10.]]\n",
      "Weights after 1 mini-batch: [[-0.01504243  0.01504243]\n",
      " [-0.01734257  0.01734257]\n",
      " [ 0.02170017 -0.02170017]]\n",
      "W2 - W1 after 1 mini-batch: [ 0.03008486  0.03468514 -0.04340035]\n",
      "Gradient/Weight:  [[-12.3371393  -12.3371393 ]\n",
      " [ -3.13033403  -3.13033403]\n",
      " [ -8.01847373  -8.01847373]]\n",
      "Weights after 2 mini-batch: [[-0.02988334  0.02988334]\n",
      " [-0.02328018  0.02328018]\n",
      " [ 0.03648348 -0.03648348]]\n",
      "W2 - W1 after 2 mini-batch: [ 0.05976667  0.04656037 -0.07296696]\n",
      "Gradient/Weight:  [[-4.96628202 -4.96628202]\n",
      " [-2.5505015  -2.5505015 ]\n",
      " [-4.05205501 -4.05205501]]\n",
      "Weights after 3 mini-batch: [[-0.03777056  0.03777056]\n",
      " [-0.03152531  0.03152531]\n",
      " [ 0.0473228  -0.0473228 ]]\n",
      "W2 - W1 after 3 mini-batch: [ 0.07554112  0.06305062 -0.0946456 ]\n",
      "Gradient/Weight:  [[-2.08819417 -2.08819417]\n",
      " [-2.61539915 -2.61539915]\n",
      " [-2.29050679 -2.29050679]]\n",
      "Weights after 4 mini-batch: [[-0.03886041  0.03886041]\n",
      " [-0.04228873  0.04228873]\n",
      " [ 0.05384615 -0.05384615]]\n",
      "W2 - W1 after 4 mini-batch: [ 0.07772082  0.08457745 -0.10769229]\n",
      "Gradient/Weight:  [[-0.28045265 -0.28045265]\n",
      " [-2.54522083 -2.54522083]\n",
      " [-1.21147839 -1.21147839]]\n",
      "Weights after 5 mini-batch: [[-0.06275195  0.06275195]\n",
      " [-0.04510495  0.04510495]\n",
      " [ 0.07376682 -0.07376682]]\n",
      "W2 - W1 after 5 mini-batch: [ 0.12550391  0.0902099  -0.14753363]\n",
      "Gradient/Weight:  [[-3.80729842 -3.80729842]\n",
      " [-0.62437082 -0.62437082]\n",
      " [-2.70049193 -2.70049193]]\n",
      "LRM Convergence Params:  [[-0.06275195  0.06275195]\n",
      " [-0.04510495  0.04510495]\n",
      " [ 0.07376682 -0.07376682]]\n",
      "training_score:  0.5807407407407408\n",
      "validation_score:  0.5740740740740741\n",
      "Starting mini BGD...\n",
      "Weights after 0 mini-batch: [-0.00703125  0.02382753 -0.00859989]\n",
      "Gradient/Weight:  [-5. -5. -5.]\n",
      "Weights after 1 mini-batch: [ 0.03008486  0.03468514 -0.04340035]\n",
      "Gradient/Weight:  [-6.16856965 -1.56516701 -4.00923687]\n",
      "Weights after 2 mini-batch: [ 0.05976667  0.04656037 -0.07296696]\n",
      "Gradient/Weight:  [-2.48314101 -1.27525075 -2.0260275 ]\n",
      "Weights after 3 mini-batch: [ 0.07554112  0.06305062 -0.0946456 ]\n",
      "Gradient/Weight:  [-1.04409708 -1.30769957 -1.1452534 ]\n",
      "Weights after 4 mini-batch: [ 0.07772082  0.08457745 -0.10769229]\n",
      "Gradient/Weight:  [-0.14022633 -1.27261041 -0.6057392 ]\n",
      "Weights after 5 mini-batch: [ 0.12550391  0.0902099  -0.14753363]\n",
      "Gradient/Weight:  [-1.90364921 -0.31218541 -1.35024596]\n",
      "Weights after 0 mini-batch: [ 0.14324985  0.10600489 -0.17015062]\n",
      "Gradient/Weight:  [-0.61940539 -0.74501256 -0.66461676]\n",
      "Weights after 1 mini-batch: [ 0.14667241  0.12524714 -0.18374575]\n",
      "Gradient/Weight:  [-0.11667342 -0.76817119 -0.36994394]\n",
      "Weights after 2 mini-batch: [ 0.15764498  0.14370816 -0.20278886]\n",
      "Gradient/Weight:  [-0.34801525 -0.64230935 -0.46953047]\n",
      "Weights after 3 mini-batch: [ 0.16371414  0.16207018 -0.21771115]\n",
      "Gradient/Weight:  [-0.18535834 -0.56648348 -0.34270859]\n",
      "Weights after 4 mini-batch: [ 0.15657594  0.18544523 -0.22579282]\n",
      "Gradient/Weight:  [ 0.22794693 -0.6302415  -0.17896198]\n",
      "Weights after 5 mini-batch: [ 0.17007601  0.20062568 -0.24528858]\n",
      "Gradient/Weight:  [-0.39688357 -0.37832769 -0.39740454]\n",
      "LR convergence_params:  [ 0.17007601  0.20062568 -0.24528858]\n",
      "training_score:  0.5807407407407408\n",
      "validation_score:  0.5740740740740741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------Connection between sigmoid and softmax------------\n",
    "############ Now set k=2, only use data from '1' and '2' \n",
    "\n",
    "#####  set labels to 0,1 for softmax classifer\n",
    "# train_X = train_X_all[train_idx]\n",
    "# train_y = train_y_all[train_idx]\n",
    "# train_X = train_X[0:1350]\n",
    "# train_y = train_y[0:1350]\n",
    "# valid_X = valid_X_all[val_idx]\n",
    "# valid_y = valid_y_all[val_idx] \n",
    "# train_y[np.where(train_y==2)] = 0\n",
    "# valid_y[np.where(valid_y==2)] = 0\n",
    "\n",
    "# test_X_all = prepare_X(raw_data_test)\n",
    "# test_y_all, test_idx = prepare_y(labels_test)\n",
    "# test_X = test_X_all[test_idx]\n",
    "# test_y = test_y_all[test_idx]\n",
    "# test_y[np.where(test_y==2)] = 0\n",
    "# visualize_features(train_X, train_y, \"2 Class Softmax train features\")\n",
    "# visualize_features(test_X, test_y, \"2 Class Softmax test features\")\n",
    "\n",
    "# ###### First, fit softmax classifer until convergence, and evaluate \n",
    "# ##### Hint: we suggest to set the convergence condition as \"np.linalg.norm(gradients*1./batch_size) < 0.0005\" or max_iter=10000:\n",
    "# ### YOUR CODE HERE\n",
    "# logisticR_classifier_multiclass = logistic_regression_multiclass(learning_rate=0.1, max_iter=10000,  k= 2)\n",
    "# logisticR_classifier_multiclass.fit_miniBGD(train_X, train_y, 32)\n",
    "# lrm_convergence_params = logisticR_classifier_multiclass.get_params()\n",
    "# print(\"LRM Convergence Params: \", lrm_convergence_params)\n",
    "# training_score = logisticR_classifier_multiclass.score(train_X, train_y)\n",
    "# print(\"training_score: \", training_score)\n",
    "# validation_score = logisticR_classifier_multiclass.score(valid_X, valid_y)\n",
    "# print(\"validation_score: \", validation_score)\n",
    "# ### END YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_X = train_X_all[train_idx]\n",
    "# train_y = train_y_all[train_idx]\n",
    "# train_X = train_X[0:1350]\n",
    "# train_y = train_y[0:1350]\n",
    "# valid_X = valid_X_all[val_idx]\n",
    "# valid_y = valid_y_all[val_idx] \n",
    "# #####       set lables to -1 and 1 for sigmoid classifer\n",
    "# train_y[np.where(train_y==2)] = -1\n",
    "# valid_y[np.where(valid_y==2)] = -1   \n",
    "\n",
    "# ###### Next, fit sigmoid classifer until convergence, and evaluate\n",
    "# ##### Hint: we suggest to set the convergence condition as \"np.linalg.norm(gradients*1./batch_size) < 0.0005\" or max_iter=10000:\n",
    "# ### YOUR CODE HERE\n",
    "# logisticR_classifier = logistic_regression(learning_rate=0.4, max_iter=10000)\n",
    "# logisticR_classifier.fit_miniBGD(train_X, train_y, 64)\n",
    "# convergence_params = logisticR_classifier.get_params()\n",
    "# print(\"LR convergence_params: \", convergence_params)\n",
    "# training_score = logisticR_classifier.score(train_X, train_y)\n",
    "# print(\"training_score: \", training_score)\n",
    "# validation_score = logisticR_classifier.score(valid_X, valid_y)\n",
    "# print(\"validation_score: \", validation_score)\n",
    "\n",
    "# LRM Convergence Params:  [[ -5.36697941   5.04173235]\n",
    "#  [-20.81503274   9.75946323]\n",
    "#  [ -3.26876102  -1.06174654]]\n",
    "# training_score:  0.9725925925925926\n",
    "# validation_score:  0.9788359788359788\n",
    "# Starting mini BGD...\n",
    "# LR convergence_params:  [10.37087186 30.43517856  2.21460478]\n",
    "# training_score:  0.9725925925925926\n",
    "# validation_score:  0.9788359788359788\n",
    "\n",
    "# lrm_convergence_params = np.array([[ -5.36697941,   5.04173235], [-20.81503274,   9.75946323], [ -3.26876102,  -1.06174654]])\n",
    "# lr_convergence_params = np.array([10.37087186, 30.43517856,  2.21460478])\n",
    "# visualize_result_multi(test_X, test_y, lrm_convergence_params)\n",
    "\n",
    "### END YOUR CODE\n",
    "\n",
    "\n",
    "################Compare and report the observations/prediction accuracy\n",
    "\n",
    "\n",
    "'''\n",
    "Explore the training of these two classifiers and monitor the graidents/weights for each step. \n",
    "Hint: First, set two learning rates the same, check the graidents/weights for the first batch in the first epoch. What are the relationships between these two models? \n",
    "Then, for what leaning rates, we can obtain w_1-w_2= w for all training steps so that these two models are \bequivalent for each training step. \n",
    "'''\n",
    "# ### YOUR CODE HERE\n",
    "train_X = train_X_all[train_idx]\n",
    "train_y = train_y_all[train_idx]\n",
    "train_X = train_X[0:1350]\n",
    "train_y = train_y[0:1350]\n",
    "valid_X = valid_X_all[val_idx]\n",
    "valid_y = valid_y_all[val_idx] \n",
    "train_y[np.where(train_y==2)] = 0\n",
    "valid_y[np.where(valid_y==2)] = 0 \n",
    "\n",
    "logisticR_classifier_multiclass = logistic_regression_multiclass(learning_rate=0.1, max_iter=1,  k= 2)\n",
    "logisticR_classifier_multiclass.fit_miniBGD(train_X, train_y, 256)\n",
    "lrm_convergence_params = logisticR_classifier_multiclass.get_params()\n",
    "print(\"LRM Convergence Params: \", lrm_convergence_params)\n",
    "training_score = logisticR_classifier_multiclass.score(train_X, train_y)\n",
    "print(\"training_score: \", training_score)\n",
    "validation_score = logisticR_classifier_multiclass.score(valid_X, valid_y)\n",
    "print(\"validation_score: \", validation_score)\n",
    "\n",
    "train_X = train_X_all[train_idx]\n",
    "train_y = train_y_all[train_idx]\n",
    "train_X = train_X[0:1350]\n",
    "train_y = train_y[0:1350]\n",
    "valid_X = valid_X_all[val_idx]\n",
    "valid_y = valid_y_all[val_idx] \n",
    "#####       set lables to -1 and 1 for sigmoid classifer\n",
    "train_y[np.where(train_y==2)] = -1\n",
    "valid_y[np.where(valid_y==2)] = -1\n",
    "\n",
    "logisticR_classifier = logistic_regression(learning_rate=0.2, max_iter=1)\n",
    "logisticR_classifier.fit_miniBGD(train_X, train_y, 256)\n",
    "convergence_params = logisticR_classifier.get_params()\n",
    "print(\"LR convergence_params: \", convergence_params)\n",
    "training_score = logisticR_classifier.score(train_X, train_y)\n",
    "print(\"training_score: \", training_score)\n",
    "validation_score = logisticR_classifier.score(valid_X, valid_y)\n",
    "print(\"validation_score: \", validation_score)\n",
    "### END YOUR CODE\n",
    "\n",
    "# ------------End------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db5f004e2c4cc8fc96863a009b6f1778bb670a3e6c944f91876f05b2b83c6909"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
